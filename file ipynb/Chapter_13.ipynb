{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b216c4d8",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886f3a9",
   "metadata": {},
   "source": [
    "Desribes the probability of an event under certain known conditions.\n",
    "\n",
    "**Establishment premise**: Each event feature is basically independent and has no relationship with other events.\n",
    "**Feature**: Not prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca44835",
   "metadata": {},
   "source": [
    "### Theory: Building a spam filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cca27e",
   "metadata": {},
   "source": [
    "#### Very simple version\n",
    "\n",
    "Use Bayes'theorem to calculate the probability of \"this email is spam\":\n",
    "```\n",
    "P(S|B)=[P(B|S)P(S)/[P(B|S)P(S)+P(B|¬S)]P(¬S)\n",
    "```\n",
    "\n",
    "* S: The event that this email is spam\n",
    "* B: The event that this email contains the word bitcoin\n",
    "\n",
    "If there are a lot of emails that are definitely spam, and a lot of emails that are definitely not spam.\n",
    "The values of P(B|S) and P(¬S) can be easily estimated.\n",
    "\n",
    "If we futher assume that the probability of an email being spam of not is half and half. (i.e. P(S)=P(¬S)=0.5)\n",
    "Then the original formula can be rewritten as:\n",
    "```\n",
    "P(S|B)=[P(B|S)/[P(B|S)P(S)+P(B|¬S)]\n",
    "```\n",
    "Assumptions:\n",
    "* 50% of spam emails contain the word bitcoin\n",
    "* 1% of non-spam emails contain the world bitcoin -> The probability that it is indeed spam is:\n",
    "```\n",
    "0.5/(0.5+0.01)=98%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8356a47",
   "metadata": {},
   "source": [
    "#### A more sphisticated version\n",
    "**Assumption**.\n",
    "* There is a vocabulary containing many words w₁,...,wɴ.\n",
    "* Let Xi represent the event that the email contains the word Wi\n",
    "\n",
    "**Assume again that we can estimate**:\n",
    "* The probability that a spam email contains wi P(Xi|S)\n",
    "* The probability that a spam email does not contain wi P(Xi|¬S)\n",
    "\n",
    "```\n",
    "P(X₁=X₁,...,Xn=xn|S) = P(X₁=x₁|S)×...×P(Xn=xn|S)\n",
    "```\n",
    "\n",
    "Assume that there are only two words in the vocabulary: \"bitcion\" and \"rolex\", and among all known spam emails:\n",
    "* Half have \"earn bitcoin\"\n",
    "* Half have \"authenic rolex\"\n",
    "\n",
    "In this case, the probability that a spam email contains both \"bitcion\" and \"rolex\" is 0.\n",
    "If we use the simple Bayesian method to estimate the probability:\n",
    "```\n",
    "P(X₁=1,X₂=1|S) = P(X₁=1|S)×P(X₂=1|S) = .5×.5 = .25\n",
    "```\n",
    "In this assumption, it is not taken into account that the two words \"bitcion\" and \"rolex\" will still affect each other.\n",
    "Rather than ideal independent events. Ignoring the gap between the ideal situation and the actual situation, this model is still often used in real-world spam filters.\n",
    "\n",
    "In practice, it is usually best to avoid multiplying many probability values.\n",
    "After multiplying multiple floating-point numbers less than zero, it is easy to cause \"**underflow**\".\n",
    "According to the algebraic relationship:\n",
    "* log(ab)=log(a)+log(b)\n",
    "* exp(log(x))=x\n",
    "\n",
    "We can convert the original formula to:\n",
    "```\n",
    "exp(log(p1)+...+log(pn))\n",
    "```\n",
    "Suppose we have a large number of marked spam and non-spam emails for training.\n",
    "(log(x))=x\n",
    "\n",
    "We can convert the original formula to:\n",
    "```\n",
    "exp(log(p1)+...+log(pn))\n",
    "```\n",
    "Suppose we have a large number of marked spam and non-spam emails for training.\n",
    "Then we can estimate the **probability P(Xi|S)** that the spam content contains the word wi\n",
    "\n",
    "However, there will still be problems in the probability calculation:\n",
    "Suppose that in all the emails we use for training, the word \"data\" in the vocabulary only appears in non-spam emails.\n",
    "```\n",
    "P(\"data\"|S)=0\n",
    "```\n",
    "This means that the classifier will classify any email containing the word \"**data**\" as **non-spam**.\n",
    "Even if there is a sentence like \"data on free bitcoin and authetic watches\".\n",
    "\n",
    "In this case, a **pseudo-count value k** is used.\n",
    "When estimating the occurrence of the word wi in spam emails, the following approach is taken:\n",
    "```\n",
    "P(Xi|S) = (k+number of emails containing the word wi in all spam emails)/(2k+number of all spam emails)\n",
    "```\n",
    "```\n",
    "P(Xi|¬S) = (k+number of emails containing the word wi in all non-spam emails)/(2k+number of all non-spam emails)\n",
    "```\n",
    "For example:\n",
    "If the word \"data\" does not appear in 98 spam emails, and the value of k is set to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb479d1",
   "metadata": {},
   "source": [
    "## Implementaton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8561e",
   "metadata": {},
   "source": [
    "### **How to create objects**\n",
    "1. Split the email content by words (tokenize)\n",
    "2. Collect different word tokens\n",
    "3. Use re.fill extract all word tokens\n",
    "4. Use **set()** to collect repeated tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b288f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "import re\n",
    "\n",
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower()                         # Convert to lowercase,\n",
    "    all_words = re.findall(\"[a-z0-9]+\", text)   # extract the words, and\n",
    "    return set(all_words)                       # remove duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb103347",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize(\"Data Science is science\") == {\"data\", \"science\", \"is\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f1b06",
   "metadata": {},
   "source": [
    "### Define data types for training data (basic objects)\n",
    "\n",
    "Since the classifier needs to continously record the word tokens and various counts and labels that appear in the training data, the object classification method is adopted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f347b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd9424",
   "metadata": {},
   "source": [
    "### Define the method to process the data type (**method**)\n",
    "\n",
    "* ham: non-spam\n",
    "* spam: spam\n",
    "\n",
    "```python\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # smooting factor\n",
    "\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "```\n",
    "\n",
    "### Define **method**: train on a bunch of emails\n",
    "**method**: train on bunch of emails\n",
    "1. Accumulate the number of spam and non-spam emails\n",
    "2. For each word token after tokenizing each email, determine whether the email is spam to determine the comulative **token_spam_counts** or **token_ham_counts**\n",
    "\n",
    "```python\n",
    "def train(self, messages: Iterable[Message]) -> None:\n",
    "    for message in messages:\n",
    "\n",
    "        # Comulative number of emails\n",
    "        if message.is_spam:\n",
    "            self.spam_messages += 1\n",
    "        else:\n",
    "            self.ham_messages += 1\n",
    "\n",
    "        # Comulative number of word occurences\n",
    "        for token in tokenize(message.text):\n",
    "            self.tokens.add(token)\n",
    "            if message.is_spam:\n",
    "                self.token_spam_counts[token] += 1\n",
    "            else:\n",
    "                self.token_ham_counts[token] += 1\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Definition **Method**: Predict the probability value of this event P(*spam*|*token*)\n",
    "That is, the probability of the email being spam when a certain token appears.\n",
    "\n",
    "**Goal**; Get P(token|spam) and P(token|ham) corresponding to each token in the vocabulary\n",
    "**Method**: Create a private auxiliary function to perform the calculation\n",
    "\n",
    "```python\n",
    "def _probabilities(self,token:str) ->Tuple[float,float]:\n",
    "\"\"\"Return P(token|spam) and P(token|ham) \"\"\"\n",
    "spam = self.token_spam_counts[token]\n",
    "ham = self.token_ham_counts[token]\n",
    "\n",
    "p_token_spam = (spam + self.k)/(self.spam_messages + 2 * self.k)\n",
    "p_token_ham = (ham + self.k)/(self.ham_messages + 2 * self.k)\n",
    "\n",
    "return p_token_spam,p_token_ham\n",
    "```\n",
    "\n",
    "### Definition **Method**: Writing prediction methods\n",
    "In calculating the final probability value, logarithmic addition is used instead of direct multiplication.\n",
    "\n",
    "```python\n",
    "def predict(self,text:str) ->float:\n",
    "text_tokens = tokenize(text)\n",
    "log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "# Iterate over each word in the vocabulary\n",
    "for token in self.tokens:\n",
    "prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "# If *token* appears in the email,\n",
    "# add the log probability of \"seeing the token\"\n",
    "if token in text_tokens:\n",
    "log_prob_if_spam += math.log(prob_if_spam)\n",
    "log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "# Otherwise add the log probability of \"not seeing the token\"\n",
    "# The log probability of \"not seeing the token\" is log(1 - the probability of seeing the token)\n",
    "else:\n",
    "log_prob_if_spam += math.log(1.0 - log_prob_if_spam)\n",
    "log_prob_if_ham += math.log(1.0 - log_prob_if_ham)\n",
    "\n",
    "prob_if_spam = math.exp(log_prob_if_spam)\n",
    "prob_if_ham = math.exp(log_prob_if_ham)\n",
    "return prob_if_spam/(prob_if_spam + prob_if_ham)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a82f79",
   "metadata": {},
   "source": [
    "### At this point, the classifier has been built!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e79208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Iterable\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k      # smoothing factor\n",
    "\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"Predict the probability value of\n",
    "        Returns P(token | spam) and P(token | ham)\"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        \"\"\"Write prediction method\"\"\"\n",
    "\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "        # Iterate over each word in the vocabulary\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # If *token* appears in the email,\n",
    "            # Add the log probability of \"seeing the token\"\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam = math.log(prob_if_spam)\n",
    "                log_prob_if_ham = math.log(prob_if_ham)\n",
    "\n",
    "            # Otherwise add the log probability of \"not seeing the token\"\n",
    "            # The log probability of \"not seeing the token\" is log(1 - the probability of seeing the token)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham = math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "\n",
    "        return prob_if_spam / (prob_if_spam * prob_if_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71451a",
   "metadata": {},
   "source": [
    "## Testing Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "337d9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some unit tests to make sure the model works correctly\n",
    "message = [Message(\"spam rules\", is_spam=True),\n",
    "            Message(\"ham rules\", is_spam=False),\n",
    "            Message(\"hello ham\", is_spam=False)]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5) # Assume smoothing factor is 0.5\n",
    "model.train(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d93fd",
   "metadata": {},
   "source": [
    "#### 1. Check whether it can calculate various couont values correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfe6235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
    "assert model.spam_messages == 1\n",
    "assert model.ham_messages == 2\n",
    "assert model.token_spam_counts == {\"spam\": 1, \"rules\": 1}\n",
    "assert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb8279",
   "metadata": {},
   "source": [
    "### 2. Make predictions\n",
    "Manually perform simple Bayesian logic calculations and confirm that the same results can be obtained:\n",
    "\n",
    "Assume the text is: \"hello spam\"\n",
    "\n",
    "* Analyze this text:\n",
    "* Number of occurences of \"spam\": 1\n",
    "* Number of occurences of \"ham\": 0\n",
    "* Number of occurences of \"rules\": 0\n",
    "* Number of occurences of \"hello\": 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9afabf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello spam\"\n",
    "\n",
    "probs_if_spam =[\n",
    "  (1+0.5)/(1+2*0.5),  #\"spam\"：有這個詞\n",
    "  1-(0+0.5)/(1+2*0.5), #\"ham\"：沒有這個詞\n",
    "  1-(1+0.5)/(1+2*0.5), #\"rules\"：沒有這個詞\n",
    "  (0+0.5)/(1+2*0.5)   #\"hello\"：有這個詞\n",
    "]\n",
    "\n",
    "probs_if_ham =[\n",
    "  (0+0.5)/(2+2*0.5),  #\"spam\"：有這個詞\n",
    "  1-(2+0.5)/(2+2*0.5), #\"ham\"：沒有這個詞\n",
    "  1-(1+0.5)/(2+2*0.5), #\"rules\"：沒有這個詞\n",
    "  (1+0.5)/(2+2*0.5)   #\"hello\"：有這個詞\n",
    "]\n",
    "\n",
    "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e20053ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.000\n",
      "0.835\n"
     ]
    }
   ],
   "source": [
    "print('%.3f'%(model.predict(text)))\n",
    "print('%.3f'%(p_if_spam/(p_if_spam+p_if_ham)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a318e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Should be about 0.83\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(text) \u001b[38;5;241m==\u001b[39m p_if_spam \u001b[38;5;241m/\u001b[39m (p_if_spam \u001b[38;5;241m+\u001b[39m p_if_ham)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Should be about 0.83\n",
    "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8ce07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
